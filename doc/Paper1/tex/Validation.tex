\section{Renderer Accuracy Validation}

There are many different renderers available to generate rendered imagery of a simulated scene.  Before using a renderer to analyze surface reconstructions a series of validation experiments should be performed to ensure that the renderer is generating imagery as expected.  These validation experiments are performed to ensure that any resultant error in an uncertainty analysis is due to SFM algorithm, not inaccurate rendering.  In this paper, we present an assessment and validation using Blender Internal Renderer, but this validation methodology could be applied to any renderer.  Note that there is no experiment presented to validate that accuracy of the lighting as the radiometric accuracy of the lighting is not the focus of this experiment.  The authors recognize the renderer could also be validated by rigorously analyzing or developing the rendering source code, but believe these experiments provide a less time consuming methodology.

\subsection{Photogrammetric Projection Accuracy}
The first validation experiment ensures that the camera interior and exterior orientation are set accurately using a pinhole camera model.  This experiment is performed by creating a simple scene consisting of a 1000m$^3$ with a 10x10 black and white checkerboard on each wall, as depicted in \figref{fig:checker}.   The black and white corner of each checkerboard corner is at a known world coordinates.  A series of images are rendered using various camera rotations, translations, focal lengths, sensor sizes, and principal point coordinates.  To ensure that the images are rendered correctly, the coordinates of the checkerboard corners are calculated from the rendered imagery using a corner feature detector and compared to the expected coordinates of the targets using photogrammetric equations.  The difference between the image derived coordinates and the photogrammetric equation derived coordinates should have a mean of 0 in both dimensions, and a subpixel variance on the order of the accuracy of the image corner feature detector.  There should also be no correlation between the accuracy of the coordinate and the location of the coordinate in the image.

\begin{figure}[H]
	\centering
	\includegraphics[height = 2in]{../fig/checkerRoom}
	\caption{A cube with a 10x10 checkerboard pattern on each wall is used to validate the photogrammetric accuracy of the Blender Internal Renderer.}
	\label{fig:checker}
\end{figure}

To validate the photogrammetric projection accuracy of the Blender Internal Renderer using this experiment, a 1000m$^3$ cube was placed with the centroid at the origin.  Five hundred images were rendered using five different interior orientations and random exterior orientations throughout the inside of the cube.  These parameters were input using the Blender Python API, and their distributions are shown in \tabref{tab:paramdist}.  The accuracy of the imagery was observed qualitatively by plotting the photogrammetric equation calculated points on the imagery in Matlab to ensure a rough accuracy.  Once the rough accuracy is confirmed, a nearest neighbor is used to develop correspondences between the Harris corner coordinates and the photogrammetric equation derived coordinates.  The mean and variance of the differences between the correspondences in each experiment are shown in \tabref{tab:deltas}.  The correlation between the difference in x, y, and radius versus several parameters shows no statistically significant correlation.  The correlation results are summarized in \tabref{tab:correlations}.

\input{tabParameterDistributions}
\input{tabDeltas}
\input{tabCorrelations}

To ensure that the variance is not an artifact of the rendering, an experiment was performed to determine the expected accuracy of the Harris Corner detector.  1000 Simulated checkerboard patterns were generated with random rotations, translations, and skew to create a synthetic image dataset.  The known coordinates of the corners were compared to the coordinates calculated with the Harris Corner feature detector, and the results are shown in in \tabref{tab:harrisdeltas}.  From these results, the hypothesis that the variance of the rendered image coordinate error is statistically different than the variance of the simulated image coordinate error is rejected.  Therefore, all the variance can be statistically attributed to the Harris Feature Corner detection algorithm, rather than the renderer. 

\input{tabHarrisDeltas}

\subsection{Point Spread Function}
The second validation experiment ensures that there is no blurring applied to the rendered image.  Specifically, this test determines that the point spread function of the rendered imagery is a unit impulse.  This test is performed by creating a white sphere placed at a distance and size such that it exists in only one pixel.  The rendered image should therefore only contain white in the one pixel and not be blurred into any other pixels.  This test is particularly important when antialiasing is performed, as the sampling and filter to combine the samples can sometimes create a blurring effect.  For example, the default antialiasing in blender uses a "distributed jitter" pattern which "ensures that a certain amount of the sample color gets distributed over the other pixels as well."  This effect can be seen in \figref{fig:aliasing}, where the intensity of the white sphere is experienced in four of the neighboring pixels, even though the sphere should only be visible in one pixel.  While this photogrammetric inaccuracy is minimal, the error could propagate into the resultant SFM derived pointcloud.

\input{figAntiAliasing}

To validate the point spread function of the Blender Internal Renderer, a sensor and scene are set up such that the geometry of the sphere is only captured with one pixel in the render.  This experiment ensures that any other pixels that contain white are an artifact of the rendering.  Rendered imagery is shown with and without antialiasing.  The antialiasing used is the default settings for the Blender Internal Renderer (8 Samples, Mitchell-Netrevali filter).  The rendered image with no antialiasing contains no blurring of the image, while the antialiased image contains a slight amount of blurring.  The antialiased imagery renders a smoother, more photorealisitic imagery, and is deemed to be suitable for experimentation. \todo{Need to word this better... I contradict myself by saying antialiasing is bad}

\subsection{Texture Resolution}
The final validation experiment ensures that any textures applied to the objects in the scene are applied in a manner which maintains the resolution of the imagery.  This validation experiment is performed by rendering a texture on a flat plane and rendering an image that contains a small number of the texture pixels.  By qualitatively looking at the image, it should be clear that the desired number of pixels are in the frame, and no smoothing is being applied.  When rendering textures in computer graphics there is an option to perform interpolation of the texture, which yields a smoother texture.  An example of a texture with and without interpolation is shown in Figure X. 

To validate the texture resolution of the Blender Internal Renderer a black and white checkerboard pattern where each checkerboard square is 1x1 texel is applied to a flat plane such that each texel represents a 10cm x 10cm square.  An image is rendered using a focal length and sensor size such that each texel is captured by 100 x 100 pixels.  The rendered image is qualitatively observed to ensure that the checkerboard is rendered for each pixel.   

\input{figtexres}
