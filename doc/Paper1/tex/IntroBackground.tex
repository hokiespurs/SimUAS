\section{Introduction}

Three dimensional Geospatial pointcloud data with an accuracy $<$5cm and with a density greater than 10 points/m$^2$ has traditionally been acquired using either terrestrial or airborne lidar, but recent advances in Structure from Motion and MultiView Stereo algorithms have enabled their use to generate pointcloud products comparable to in density and accuracy to lidar data.  SFM and MVS algorithms began development 30+ years ago (site\todocite{cite: OLD SFM-MVS}) but have only begun to be utilized for commercial surveying applications recently due to the advances in camera hardware, Unmanned Aerial Systems (UAS), computer processing power, and commercial SFM-MVS software.  Research into SFM and MVS in the geomatics community is currently focused on both the accuracy and potential applications of the commercial SFM and MVS software packages such as Agisoft Photoscan and Pix4d \todocite{cite: Geomprph sfm paper}.  The accuracy of SFM-MVS can vary greatly depending on a variety of factors \todocite{cite: accuracy vs factors}, and the geomatics community is currently focused on determining potential use cases where SFM-MVS derived pointclouds can begin to replace lidar as an alternative surveying tool, without sacrificing accuracy.  \todo{Should I cite specific use examples?}.  The most common methodology for assessing the use cases and accuracy of SFM-MVS algorithms is to collect data in the field using a UAS and compare the data to lidar data.  It is difficult to constrain many of the independent variables using this methodology, and the data acquisition can be expensive and time consuming.  We propose a computer graphics based workflow to simulate various scenes and maintain full control over the ground-truth and the camera parameters.  This workflow will allow geomatics engineers to perform more robust experiments to assess the feasibility and accuracy of SFM-MVS in various applications.

The 3D reconstruction methods used in most commercial software consist of an SFM algorithm first to solve for camera exterior and interior orientations, then an MVS algorithm to increase the density of the pointcloud.  Unordered photographs are input into the software, and a keypoint detection algorithm such as SIFT\todocite{cite sift} is used to detect keypoints and correspondences between images.  The SFM algorithm solves for camera interior orientation, exterior orientation, and generates a "sparse" pointcloud.  A bundle adjustment is performed to optimize the matches.  Without any additional information, the coordinate system is arbitrary in translation and rotation with an inaccurate scale.  To further constrain the problem and develop a georectified pointcloud Ground Control Points(GCPs) and/or Initial Camera GPS Positions are used to constrain the solution.  A camera calibration file can also be input to reduce the number of parameters that are solved for, however only inputting a camera calibration file will only help resolve the scale of the pointcloud coordinate system, not the absolute translation and rotation.  These input data can be used to generate an absolute coordinate system either with a Helmert transformation after the pointcloud is generated, \todocite{cite: helmert uas} or using a nonlinear optimization within the bundle adjustment.  The nonlinear optimization will generate more accurate results\todocite{cite}.  The interior and exterior orientation for each image is used as the input to the MVS algorithm, which generates a more dense pointcloud.  The MVS algorithm can generate more correspondences because it utilizes a 1D search along the epipolar line between two images due to the known interior and exterior orientation\todo{make sure this is true}.  For this reason, the accuracy of the MVS algorithm is highly dependent on the accuracy of the parameters calculated with the SFM algorithm.  A detailed explanation of the various MVS algorithms can be found in paper\todocite{cite MVS algorithms}.  Each of these algorithms also assumes that the scene is rigid with constant lambertian surfaces, and deviations from these assumptions will effect the accuracy. 

Numerous studies have been performed to quantify the accuracy of the SFM-MVS algorithms in a variety of environments\todocite{cite lidar accuracy studies}.  The most common and robust method has been to compare the SFM-MVS derived pointcloud to a groundtruth terrestrial lidar survey \todocite{cite: sfm vs lidar}.  This comparison can exhibit errors in vegetated areas due to vegetation moving by blowing in the wind, and due increased uncertainty in the lidar pointcloud.  Independent GPS control points have also been used, but result in fewer correspondences to compare with \todocite{cite: sfm vs independent gcps}.  The use of independent control points also can exhibit a bias when the points used are easily photo identifiable targets (eg. checkerboards).  The highly textured independent control points will demonstrate a much better accuracy than a homogeneous surface, due to the lack of matching features, and therefore are not necessarily representative of the accuracy of the entire scene.  The accuracy of SFM is adversely affected by: homogeneous scene texture, poor image overlapping, lens distortion not fully modeled by the nonlinear lens distortion equation, poor GCP distribution, inaccurate GCP or camera positions, poor image resolution, blurry imagery, noisy imagery, varying sun shadows, moving objects in the scene, user error in manually selecting image coordinates of GCPs, low number of images, or a low number of GCPs\todocite{cite: sfm error sources}.  With a computer graphics workflow, each of these variables can be constrained and varied in order to assess the effect on the overall accuracy of the scene.  The accuracy of the resultant SFM-MVS pointcloud is also assessed by comparing it to an ideal groundtruth dataset with no uncertainty because it is all simulated data.  

\section{Computer Graphics for Remote Sensing Analysis}

The field of computer graphics was first developed in the 19XXs \todocite{origin of CGI date} and the advancement of the field has been predominantly driven by the desire to create more realistic video game and movie special effects. \todocite{cite: computer graphics origins}  The package of software and algorithms to turn a simulated scene with various textures, material properties, and lighting into an image or sequence of images is called a render engine.  While there are many different render engines available with different algorithms, they all follow a basic workflow, or computer graphics pipeline.  

First, A 3D scene is generated using vertices, faces, and edges.  For most photo-realistic rendering, meshes are generated using an array of either triangular surfaces or quadrilateral surfaces to create objects.  Material properties are applied to each of the individual surfaces to determine the color of the object.  Most software allow for the user to set a diffuse, specular, and ambient light coefficients as well as their associated colors in order to specify how light will interact with the object.  The coefficient specifies how much diffuse, specular, and ambient light is reflected off of the surface of the object, while the color specifies the amount of red, green, and blue light that is reflected from the surface.  The color using the material properties is only associated with each plane in the mesh, so for highly detailed coloring on objects there would need to be a large amount of faces.  The more efficient way of creating detailed colors on an object without increasing the complexity of the surface of the objects is to add a "texture" to the object.  A texture is an image which is overlaid on the mesh in a process called u-v mapping.  The computer graphics definition for a "texture" object, is not to be confused with the SfM and photogrammetry definition, which is the level of detail and unique photo identifiable features in an image.

Once a scene is populated with objects and their associated material and texture properties, light sources and shading algorithms must be applied to the scene.  The simplest method is to set an object material as "shadeless", which eliminates any interaction with light sources and will render each surface based on the material property and texture with the exact RGB values that were input.  The more complex and photorealistic method is to place light sources in the scene.  Each light source can be set to simulate different patterns and angles of light rays with various levels of intensity and range based intensity falloff.  Most render engines also contain shadow algorithms, which perform ray tracing to determine what regions are shadowed by an object.  Once a scene is created with light sources and shading parameters set, simulated cameras are placed to create renders of the scene. The camera translation, rotation, sensor size, focal length, and principal point are input and a pinhole camera model is used.  The rendering algorithm generates a 2D image of the scene using the camera position and all of the material properties of the objects. The method, accuracy, and performance of generating this 2D depiction of the scene is where most render engines differ. 

There are many different rendering methodologies, but the one chosen for this research is Blender Internal Render Engine which is a rasterization based engine.  The algorithm determines which parts of the scene are visible to the camera, and assigns each object color to the pixel samples.  This algorithm is fast, but is unable to perform some of the more advanced rendering features such as global illumination and motion blur.  A more detailed description of common rendering engines and algorithms can be found XX\todocite{cite: render engine summary}.

\begin{itemize}
	\item Reference Others who have used CGI for remote sensing (dirsig, etc) \todocite{CGI photogrammetry }
\end{itemize}