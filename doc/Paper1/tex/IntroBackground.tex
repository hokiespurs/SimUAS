\section{Introduction}

Three dimensional Geospatial pointcloud data with an accuracy $<$5cm and with a density greater than 10 points/m$^2$ has traditionally been acquired using either terrestrial or airborne lidar, but recent advances in Structure from Motion and MultiView Stereo algorithms have enabled their use to generate pointcloud products comparable to in density and accuracy to lidar data.  SFM and MVS algorithms began development 30+ years ago (site\todocite{cite: OLD SFM-MVS}) but have only begun to be utilized for commercial surveying applications recently due to the advances in camera hardware, Unmanned Aerial Systems (UAS), computer processing power, and commercial SFM-MVS software.  Research into SFM and MVS in the geomatics community is currently focused on both the accuracy and potential applications of the commercial SFM and MVS software packages such as Agisoft Photoscan and Pix4d \todocite{cite: Geomprph sfm paper}.  The accuracy of SFM-MVS can vary greatly depending on a variety of factors \todocite{cite: accuracy vs factors}, and the geomatics community is currently focused on determining potential use cases where SFM-MVS derived pointclouds can begin to replace lidar as an alternative surveying tool, without sacrificing accuracy.  \todo{Should I cite specific use examples?}.  The most common methodology for assessing the use cases and accuracy of SFM-MVS algorithms is to collect data in the field using a UAS and compare the data to lidar data.  It is difficult to constrain many of the independent variables using this methodology, and the data acquisition can be expensive and time consuming.  We propose a computer graphics based workflow to simulate various scenes and maintain full control over the ground-truth and the camera parameters.  This workflow will allow geomatics engineers to perform more robust experiments to assess the feasibility and accuracy of SFM-MVS in various applications.

The 3D reconstruction methods used in most commercial software consist of an SFM algorithm first to solve for camera exterior and interior orientations, then an MVS algorithm to increase the density of the pointcloud.  Unordered photographs are input into the software, and a keypoint detection algorithm such as SIFT\todocite{cite sift} is used to detect keypoints and correspondences between images.  The SFM algorithm solves for camera interior orientation, exterior orientation, and a "sparse" pointcloud.  A bundle adjustment is performed to optimize the matches.  Without any additional information, the coordinate system is arbitrary in translation, rotation, and scale.  To further constrain the problem and develop a georectified pointcloud Ground Control Points and/or Initial Camera GPS Positions are used to constrain the solution.  A camera calibration file can also be input to reduce the number of parameters that are solved for, however only inputting a camera calibration file will only help resolve the scale of the pointcloud coordinate system, not the absolute translation and rotation.  These input data can be used to generate an absolute coordinate system either be done with a Helmert transformation after the pointcloud is generated, \todocite{cite: helmert uas} or using a nonlinear optimization within the bundle adjustment.  The nonlinear optimization will generate more accurate results.  The interior and exterior orientation for each image is used as the input to the MVS algorithm, which generates a more dense pointcloud.  The MVS algorithm can generate more correspondences because it utilizes a 1D search along the epipolar line between two images due to the known interior and exterior orientation.  For this reason, the accuracy of the MVS algorithm is highly dependent on the accuracy of the parameters calculated with the SFM algorithm.  A detailed explanation of the various MVS algorithms can be found in paper\todocite{cite MVS algorithms}.  Each of these algorithms also assumes that the scene is rigid with constant lambertian surfaces, and deviations from these assumptions will drastically effect the accuracy. 

Numerous studies have been performed to quantify the accuracy of the SFM-MVS algorithms in a variety of environments\todocite{cite lidar accuracy studies}.  The most common and robust method has been to compare the SFM-MVS derived pointcloud to a groundtruth terrestrial lidar survey \todocite{cite: sfm vs lidar}.  Independent GPS control points have also been used, but result in fewer correspondences to compare with \todocite{cite: sfm vs independent gcps}.  The use of independent control points also can exhibit a bias when the points used are easily photo identifiable targets (eg. checkerboards).  The highly textured independent control points will demonstrate a much better accuracy than a homogeneous surface, due to the lack of matching features, and therefore are not necessarily representative of the accuracy of the entire scene.  The accuracy of SFM is adversely affected by: homogeneous scene texture, poor image overlapping, lens distortion not fully modeled by the nonlinear lens distortion equation, poor GCP distribution, inaccurate GCP or camera positions, poor image resolution, blurry imagery, noisy imagery, varying sun shadows, moving objects in the scene, user error in manually selecting image coordinates of GCPs, low number of images, or a low number of GCPs.  With a computer graphics workflow, each of these variables can be constrained and varied in order to assess the effect on the overall accuracy of the scene.  The accuracy of the resultant SFM-MVS pointcloud is also assessed by comparing it to a scene with no uncertainty because it is simulated.  

\section{Computer Graphics for Remote Sensing Analysis}

The field of computer graphics was first developed in the 19XXs and the advancement of the field has been predominantly driven by the desire to create more realistic video game and movie special effects. \todo{cite: computer graphics origins}  The package of software and algorithms to turn a simulated scene into an image or movie is called a renderer.  While there are many different renderers available with different algorithms, they all follow a basic workflow, or computer graphics pipeline.  

First, A 3D scene is generated using vertices, faces, and edges.  For most photo-realistic rendering, meshes are generated using an array of either triangular surfaces or quadrilateral surfaces to create objects.  Material properties are applied to each of the individual surfaces to determine the color of the object.  Most software allow for the user to set a diffuse, specular, and ambient light coefficients as well as their associated colors in order to specify how light will interact with the object.  The coefficient specifies how much diffuse, specular, and ambient light is reflected off of the surface of the object, while the color specifies the amount of red, green, and blue light that is reflected from the surface.  The color using the material properties is only associated with each plane in the mesh, so for highly detailed coloring on objects there would need to be a large amount of faces.  The more popular way of creating detailed colors on an object is to add a "texture" to the object.  A texture is an image which is overlaid on the image in a process called u-v mapping.  The computer graphics definition for "texture", is not to be confused with the SfM and photogrammetry definition, which is the level of detail and unique photo identifiable features in an image.

Once a scene is populated with objects and their associated material and texture properties, light sources and shading algorithms must be applied to the scene.  The simplest method is to set an object material as "shadeless", which eliminates any interaction with light sources and will render it as a constant color or texture.  The more complex and photorealistic method is to place light sources in the scene.  Each light source can be set to simulate different patterns and angles of light rays with various levels of intensity and range based intensity falloff.  Most renderers also contain shadow algorithms, which perform ray tracing to determine what regions are shadowed by an object.  Once a scene is created with light sources and shading parameters set, cameras are placed to create renders of the scene.  The camera is set using exterior and interior orientations, though with slightly different terminology.  The rendering algorithm generates a 2D image of the scene using the camera position and all of the material properties of the objects.  The method and performance of generating this 2D depiction of the scene is where most renderers differ. 

There are many different rendering methodologies, but the one chosen for this research is Blender Internal Renderer which is a rasterization engine.  The algorithm determines which parts of the scene are visible to the camera, and assigns each object color to the scene.  This algorithm is very fast, but is unable to perform some of the more advanced rendering features such as global illumination and motion blur.  A more detailed description of common rendering engines and algorithms can be found XX.

\begin{itemize}
	\item Reference Others who have used CGI for remote sensing (dirsig, etc)
\end{itemize}