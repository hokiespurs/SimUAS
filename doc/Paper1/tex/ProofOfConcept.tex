\section{Proof of Concept}

An example experiment was generated as a proof of concept to demonstrate a potential workflow for testing the effect of various independent variables on SfM accuracy.  

A 100m x 100m DSM was manually generated in Blender using N vertices and N unique triangular planes to represent a hilly surface.  The specular response of the surface was defined as perfectly lambertian, and the color of the surface was generated from two image textures.  The first image texture is a XX Mp seamless grass image, that was taken from a UAV at XXX.  This texture is repeated 5 times in both the X and Y dimensions, for a texel ground spacing of XX cm.  The second texture is a XMp image taken from a UAV from at xxx.  This texure is not repeated, and therefore has a texel ground spacing of XX cm.  These images are mosaiced together using an intensity value of X for the repeating grass texture, and an intensity value of Y for the overview image.  Both textures are interpolated in the renderer to avoid crisp edges or corners in between texels.  A large 5m x 5m cube is placed in the center of the scene to observe the inability of SfM to generate crisp corners.  The cube is textured using a XMp image of asphalt that was taken by a DSLR.  The texel ground spacing is XX cm.  Eleven 0.5m x 0.5m Black and White Checkered, lambertian GCPs are distributed in the scene to simulate being laid on the simulated hilly surface in an adhoc manner.  To illuminate the scene, the sun is placed at a nadir angle for the first image, and its angle is linearly interpolated about the x axis so that the final image is rendered with a 30 degree solar incident angle.  The ray tracing algorithm is set up so that shadows are cast by objects within the scene.  An ambient light source for the environment is set to be 10\% the value of the solar illumination, to prevent shadowed areas from rendering completely black.  

A trajectory is generated in Matlab to simulate the motion of a small action camera on a UAS at an altitude of 25 meters flying a grid pattern across the scene with X sidelap and X overlap.  To avoid imaging the edge of the simulated hilly topography, the outermost 10 meters of the scene are not considered in the simulated flight planning.  The camera pose is nadir, but with a $\pm 5 \degree$ random offset in roll, pitch, and yaw angles of the uas.  The Blender Internal Render was used to render 100 images, which took approximately X minutes on a XX computer.  To simulate more nonidealized imagery, the raw images were postprocessed to add gaussian blur, gaussian noise, salt and pepper noise, and nonlinear distortion governed by the brown distortion model using a Matlab script.

The postprocessed imagery was loaded into Agisoft Photoscan (version XX), and processed with settings shown in Table X.  Notice that the GCP positions, camera positions, and camera interior orientation are input with no uncertainty, since the scene has been simulated.  Additionally, the pixel coordinates of the GCPs, which are traditionally clicked by the user by varying degrees of accuracy are calculated using photogrammetric equations and input into the program.  The accuracy of the control points as reported by Photoscan are shown in Table X.  Note that as a result of the unrealistic accuracy and of the data input into the program, the errors are reported in millimeters.  This is not realistic to expect from a real world scenario, but provides an example best case scenario for what to expect from SfM. The result of the processing is an orthophoto, with 1cm resolution, a sparse pointcloud with N points, and a dense pointcloud with N points.  These data are then compared to the groundtruth data, which in this case is the model and a simulated orthophoto from Blender.  

Cloud Compare is used to compare the points to the mesh using the points to plane tool.  The resultant accuracies of the sparse and dense pointcloud are exported to Matlab, and gridded using an IDW .  The variance of the accuracy is also visualized by binning the accuracy values into the nearest grid node and calculating the variance of the values in each node.  These data are visualized in Figure X and summarized in Table X.  

