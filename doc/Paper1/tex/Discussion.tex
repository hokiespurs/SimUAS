\section{Discussion}

The use case demonstration provides an example of the type of rigorous analysis that can be obtained by utilizing computer generated imagery, rather than field data.  It is important to note that the data and results associated in this experiment are closely coupled to the texture and topography of the scene.  Future work will vary these independent variables in order to assess their effect on pointcloud accuracy.

The first conclusion from this experiment is that the error and standard deviation of error are larger for points outside of the area of interest, which in this experiment was -50m to 50m in both the x and y directions.  This is shown in the spatial error plot in \figref{fig:spatial}.  The cause of this error is the poor viewing geometry of these points, where they are only seen by a few cameras at oblique angles.  In practice, these points should be included in the final data product with caution, as it is shown here that the errors can be significantly greater than in the AOI.  

The second conclusion from this experiment is that a higher quality dense pointcloud results in a more accurate pointcloud, as shown qualitatively in \figref{fig:boxplot} and quantitatively in \figref{fig:hist}.  The quality settings in Photoscan determines the amount of downsampling of the imagery that should occur before performing the reconstruction algorithm.  The downsampling of the imagery removes some of the finer texture details in the imagery, and therefor reduces the quality of the keypoint matching.  The authors recommend using the highest quality setting that the computer processing the dataset can handle.  For this experiment, a relatively small number of images (77) were used to create the dense pointcloud, which took almost 12 hours for the highest pointcloud setting.  The resultant pointcloud for this setting also contained 186 Million points, which caused some pointcloud data viewers and processing to fail due to memory issues.  For this reason, ultra high may not be a possible solution for all experiments.  

The third conclusion is that the RMSE of the GCP control network as shown in Agisoft Photoscan is insufficient to characterize the accuracy of the resultant dense pointcloud.  In this extremely idealized experiment, where the GCP positions, pixel coordinates of GCPs, camera positions, and camera calibration were all input precisely, the GCP control network 3D RMSE was 0.38mm.  The smallest standard deviation of the pointcloud error, using the "ultra high" quality setting, is 2.6mm and the largest standard deviation, using the "lowest" setting, is  32.3mm, as shown in \tabref{tab:pctime}.  Further experimentation is needed to determine the relationship between the GCP total RMSE and the statistics of the dense pointcloud, and this workflow is well suited to perform the experimentation required to make accurate generalized conclusions with confidence.  


\subsection{Methodology Implications}
This methodology generates photogrammetrically accurate imagery rendered using a pinhole camera model of a scene with various textures and lighting  which is then processed to assess SfM pointcloud accuracy.  The rendered imagery can be postprocessed to add noise, blur, nonlinear distortion, and other effects to generate imagery more reminiscent of that from a real world scenario.  The accuracy of the camera trajectory, GCP position, camera calibration, and GCP pixel coordinates in each image can also be systematically adjusted to simulate uncertainty in a real world scenario.  The ability to adjust these parameters enables a user to perform a sensitivity analysis with numerous combinations of independent variables. 

While this methodology enables the user to perform repeatable, accurate experiments without the need for time consuming field work, there are currently some limitations in the experiment methodology when utilizing the Blender Internal Render Engine.  First, the internal render engine does not handle global illumination, and therefore light interactions between objects are not modeled.  A second limitation of the lighting schema is that the radiometric accuracy has also not been independently validated.  There are a few methods within the render engine which effect the exposure and gamma of the resultant imagery. For this experiment, these exposure and gamma values were set at the default and provided imagery that was not over or underexposed. While the lighting in the scene using the Blender Internal Render Engine does not perfectly replicate physics based lighting, the absolute color of each surface of an object is constant and perfectly lambertian.  The keypoint detection and SfM algorithms utilize gradients in colors and the absolute colors of the scene, and the accuracy should not be effected by this inaccuracy.

Another more broad source of inaccuracy in the Blender Internal Render Engine methodology is the methodology to convert the scene to pixel values relies in an integration over a finite number of subpixel super-sampling.  This deviates from a real world camera where the pixel value is a result of an integration over all available light.  The Blender Internal Render Engine uses the term "antialiasing" to describe a supersampling methodology for each pixel, which can supersample a up to 16 samples per pixel.  This small, finite number of samples per pixel can induce a small amount of inaccuracy when mixed pxiels are present.  These small inaccuracies, though, are small enough to be deemed negligible for this experiment.  

A potential source of uncertainty induced into the system is the use of repeating textures to generate a scene.  In the use case provided in Section X\todo{fix}, the grass texture is repeated 10 times in both the x and y direction.  This repeating pattern was overlaid onto another image, to create different image color gradients in an attempt to generate unique texture features.  Despite this effort, it is possible that keypoint detection and matching algorithms could generate false positives which may bias the result if not removed as outliers.  In this experiment, this effect was not observed, but if the scene is not generated carefully, these repeating textures could induce a significant amount of inaccuracy in the SfM processing step.
